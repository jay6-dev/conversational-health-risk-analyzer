{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzs1vBYc/8/a04//HUL6hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay6-dev/conversational-health-risk-analyzer/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-nApJXAsQXR",
        "outputId": "ac97f9ba-138b-4808-8cf8-02c1d4a94ee7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of processed text chunks (first 5):\n",
            "Chunk 1: 2023 art clinical guidelines\n",
            "for the management of hiv in adults, pregnancy  \n",
            "and breastfeeding, adolescents, children, infants  \n",
            "and neonates\n",
            "june 2023 version 3\n",
            "republic of south africa national dep...\n",
            "Chunk 2: __________________________________________________________ 2\n",
            "overview ___________________________________________________________________________ 3\n",
            "the goals of art 3\n",
            "art eligibility  ________________...\n",
            "Chunk 3: the principal \n",
            "goal of art is to attain and maintain viral suppression, which will \n",
            "prevent new hiv infections, increase life expectancy, decrease \n",
            "morbidity and mortality as well as improve the quali...\n",
            "Chunk 4: the “test and treat all” approach has allowed people living with \n",
            "hiv (plhiv) to access art timeously....\n",
            "Chunk 5: south is committed to using available technology and evidence \n",
            "to continue the fight against hiv....\n",
            "Total number of chunks: 569\n",
            "Searching for: 'what is hiv'\n",
            "No relevant snippets found for this query.\n",
            "\n",
            "--- Basic Chatbot Interaction ---\n",
            "Enter your question about the PDF, or type 'exit' to quit.\n",
            "No relevant information found for 'what hiv' in the document.\n",
            "\n",
            "No relevant information found for 'what is hiv management?' in the document.\n",
            "\n",
            "\n",
            "Found 82 relevant snippets for 'hiv':\n",
            "Snippet 1: 2023 art clinical guidelines\n",
            "for the management of hiv in adults, pregnancy  \n",
            "and breastfeeding, adolescents, children, infants  \n",
            "and neonates\n",
            "june 2023 version 3\n",
            "republic of south africa national dep...\n",
            "Snippet 2: __________________________________________________________ 2\n",
            "overview ___________________________________________________________________________ 3\n",
            "the goals of art 3\n",
            "art eligibility  ________________...\n",
            "Snippet 3: the principal \n",
            "goal of art is to attain and maintain viral suppression, which will \n",
            "prevent new hiv infections, increase life expectancy, decrease \n",
            "morbidity and mortality as well as improve the quali...\n",
            "Snippet 4: the “test and treat all” approach has allowed people living with \n",
            "hiv (plhiv) to access art timeously....\n",
            "Snippet 5: south is committed to using available technology and evidence \n",
            "to continue the fight against hiv....\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "# The following imports are removed as they depend on OpenAI or the RAG chain\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. Load the PDF document\n",
        "PDF_PATH = \"/content/National ART Clinical Guideline 2023_06_06 version 3 Web.pdf\"\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "documents = loader.load()\n",
        "\n",
        "# 2. Split the document into chunks (this is still useful for context if needed)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Removed: 3. Create embeddings and a vector store (requires API key)\n",
        "# Removed: 4. Define the RAG prompt template\n",
        "# Removed: 5. Set up the Language Model (LLM) and RAG chain\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# --- NLTK SECTION ---\n",
        "# ------------------------------------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "nltk.download('stopwords') # Download stopwords for improved query processing\n",
        "from nltk.corpus import stopwords # Import stopwords\n",
        "\n",
        "# Extract raw text for NLTK section\n",
        "extracted_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "# 1. Convert to lowercase\n",
        "lowercase_text = extracted_text.lower()\n",
        "\n",
        "# 2. Split into sentences\n",
        "processed_text_chunks = nltk.sent_tokenize(lowercase_text)\n",
        "\n",
        "print(\"Sample of processed text chunks (first 5):\")\n",
        "for i, chunk in enumerate(processed_text_chunks[:5]):\n",
        "    print(f\"Chunk {i+1}: {chunk[:200]}...\")\n",
        "print(f\"Total number of chunks: {len(processed_text_chunks)}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# --- QUERY FUNCTION ---\n",
        "# ------------------------------------------------------------\n",
        "def query_pdf(query, text_chunks):\n",
        "    lowercase_query = query.lower()\n",
        "    # Tokenize the query words\n",
        "    query_words = nltk.word_tokenize(lowercase_query)\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Filter out stopwords and non-alphanumeric words from the query\n",
        "    significant_query_words = [word for word in query_words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    print(f\"DEBUG: Query: '{query}'\")\n",
        "    print(f\"DEBUG: Significant query words: {significant_query_words}\")\n",
        "\n",
        "    relevant_snippets = []\n",
        "    if not significant_query_words: # Handle queries that are all stopwords or non-alphanumeric\n",
        "        print(\"DEBUG: No significant query words found after filtering.\")\n",
        "        return []\n",
        "\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        # Tokenize the chunk into words for proper word-based matching\n",
        "        chunk_words_raw = nltk.word_tokenize(chunk)\n",
        "        # Create a set of alphanumeric words from the chunk for efficient lookup\n",
        "        chunk_word_set = set(cw_raw for cw_raw in chunk_words_raw if cw_raw.isalnum())\n",
        "\n",
        "        all_words_found = True\n",
        "        # Optional: Print chunk_word_set for first few chunks to debug\n",
        "        if i < 5:\n",
        "            print(f\"DEBUG: Chunk {i} word set: {list(chunk_word_set)[:10]}...\")\n",
        "\n",
        "        for word in significant_query_words:\n",
        "            if word not in chunk_word_set:\n",
        "                all_words_found = False\n",
        "                # Optional: Print if a significant word is not found in a chunk\n",
        "                if i < 5:\n",
        "                    print(f\"DEBUG:   Significant word '{word}' NOT found in chunk {i} ({chunk[:50]}...)\")\n",
        "                break\n",
        "\n",
        "        if all_words_found:\n",
        "            # Optional: Print if all words are found in a chunk\n",
        "            if i < 5:\n",
        "                print(f\"DEBUG:   All significant words found in chunk {i} ({chunk[:50]}...)\")\n",
        "            relevant_snippets.append(chunk)\n",
        "    return relevant_snippets\n",
        "\n",
        "# Test the function\n",
        "sample_query = \"what is hiv\"\n",
        "found_snippets = query_pdf(sample_query, processed_text_chunks)\n",
        "\n",
        "print(f\"Searching for: '{sample_query}'\")\n",
        "if found_snippets:\n",
        "    print(f\"Found {len(found_snippets)} relevant snippets:\")\n",
        "    for i, snippet in enumerate(found_snippets[:5]):\n",
        "        print(f\"Snippet {i+1}: {snippet[:200]}...\")\n",
        "else:\n",
        "    print(\"No relevant snippets found for this query.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# --- BASIC CHATBOT INTERACTION ---\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n--- Basic Chatbot Interaction ---\")\n",
        "print(\"Enter your question about the PDF, or type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"\\nYour Question: \")\n",
        "\n",
        "    if user_question.lower() == 'exit':\n",
        "        print(\"Exiting chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    found_snippets = query_pdf(user_question, processed_text_chunks)\n",
        "\n",
        "    if found_snippets:\n",
        "        print(f\"\\nFound {len(found_snippets)} relevant snippets for '{user_question}':\")\n",
        "        for i, snippet in enumerate(found_snippets[:5]):\n",
        "            print(f\"Snippet {i+1}: {snippet[:200]}...\")\n",
        "    else:\n",
        "        print(f\"No relevant information found for '{user_question}' in the document.\\n\")"
      ]
    }
  ]
}